{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "130837b6fb4929d1f10d5dc08a7bb12f35bf0c2b0034c66eba43f79388a27fad"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multiple Form 13 Filings Proof of Concept"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The goal of this is to write a program that downloads the form 13 filings from hedge funds and exports them in a csv file. This csv file can then be used to drive other analysis. The csv should contain the last 4 years of filings, this means there will be 12 dates present. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "source": [
    "The code can be pulled from the sec website using the bs4 scraping program that is already written. The various periods and url links can be stored in a dict where they can be referenced by the program. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Notes\n",
    "> It is important to note that there is a more elegant solution to the problem of matching the raw output from the SEC edgar website to the associated ticker. In my opinion, this is the use of CUSIP numbers, these are listed on the SEC filings. The problem however is that CUSIP numbers are not publically available. The ideal system would be a .csv with CUSIP numbers, associated company name, and stock symbol. This would allow fast and highly accurate matching with little duplicate datapoints. I was not able to use this approach because CUSIP data is expensive. One problem with my approach is the required level of matching accuracy. (0.70 for the function below) This number determines whether a match is included in the result based on the percent match to the exchange company names. Through trial and error, I have found that 0.70 works well because it eliminates most of the incorrect matches but allows the code to approximate most of the correct matches as well. In the example below, there is a duplicate values in ABEO, this is due to the NLP matching. If the accuracy benchmark (currently set at 0.70) is decreased to 0.60, it excludes datapoints that were correctly generated without removing the duplicate value of ABEO. This basically means that one company name is close enough to that of another name that they are matched incorrectly with a higher degree of accuracy than some names that are matched correctly. This is a fundamental risk to the use of NLP and TDIFD for name matching. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### *The code below is the modified ticker matching function, the modifications include processing the tickers to remove duplicates, sum those values, and remove companies that are associated with puts.* "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" These .csv files were downloaded from the NASDAQ and NYSE websites and are public \"\"\"\n",
    "\n",
    "# this is the creation of the df that the functions will reference\n",
    "nasdaq = pd.read_csv('NASDAQ.csv') #imports the NASDAQ tickers\n",
    "nyse = pd.read_csv('NYSE.csv') #imports the NYSE tickers\n",
    "combEx = nasdaq.append(nyse,ignore_index=True) #this is the combination of the data from the nasdaq and nyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the version of the pull urls from sec code that will be used in this program. important to note that since book values are being included, ticker nlp matches that are below the accuracy level need to be replaced with error or another identifier so that the corresponding book value pct can also be removed using list comp. \n",
    "\n",
    "def pullSec2(url):\n",
    "    \"\"\" pulls both the name of the company and the book value from the sec \"\"\"\n",
    "    names = []\n",
    "    book_value = []\n",
    "    source = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "    FormData = soup.find_all('td', class_=\"FormData\")\n",
    "    for xx in FormData[0:]:\n",
    "        res = xx.text.strip()\n",
    "        names.append(res) # creates list of raw outputs, un-uniform outputs, contains other info\n",
    "\n",
    "    FormDataR = soup.find_all('td', class_=\"FormDataR\")\n",
    "    for aa in FormDataR[0:]:\n",
    "        res1 = aa.text.strip()\n",
    "        book_value.append(res1)\n",
    "    book_value = book_value[0::5] # list of strings that contain the values of the positions \n",
    "\n",
    "    \"\"\" DATA PROCESSING \"\"\"\n",
    "\n",
    "    \"\"\" Creating list of Company names \"\"\" # if a Put, then Put is added to the list after the name of the company. \n",
    "    cleanNames = [a for a in names if a != 'Put' and a != 'Call'] # removes options from the raw output, this is to create uniform ordering\n",
    "    cleanNames = cleanNames[0::5] # selects the company names from the list based on location in list\n",
    "    namesDeriv = [a for a in names if a in cleanNames or a == 'Put'] # pulls the company names and options out of the list in the correct order, if name associated with Put then Put added to list after the name of the company \n",
    "    \"\"\" Replacing names associated with Put options with 9999 \"\"\" # this is done to preserve the len of the list and keep it equal to the len of the list of values \n",
    "    names = []\n",
    "    for i in range(0,len(namesDeriv)-1): # iterate through index of the list\n",
    "        if namesDeriv[i+1] == 'Put': # check if next item in list is put or if current item in list is, \n",
    "            names.append(9999) # if the next item in the list is Put, then append 9999 to the list, this takes the place of the company name associated with the Put \n",
    "        elif namesDeriv[i+1] != 'Put' and namesDeriv[i] != 'Put': # if company does not have put after it then add to list, not allow str \"puts\" to be appended \n",
    "            names.append(namesDeriv[i])\n",
    "\n",
    "    \"\"\" Note the method above does not count the last item in the list, the code below fixes this \"\"\"\n",
    "    if namesDeriv[len(namesDeriv)-1] != 'Put': # checks if last item in the list is a Put\n",
    "        names.append(namesDeriv[len(namesDeriv)-1]) # if last item in the list is not a Put, then add last item to the list\n",
    "\n",
    "    \"\"\" Creation of Pct Book values and forming tuples \"\"\"\n",
    "    intValues = [int(a.replace(',','')) for a in values] # removes commas from strings in list, converts to int \n",
    "    bookPct = [round(a/sum(intValues),3) for a in intValues] # creates new list of each value as a percentage of the total\n",
    "    tuples = list(zip(names,bookPct)) # this is a list of tuples that contain the data \n",
    "    tuples = [(a,b) for a,b in tuples if a != 9999] # this removes the names and values that had puts and were replaced with 9999, tuple was created so that book the associated book value could be removed with the 9999 list entry. \n",
    "\n",
    "    \"\"\" There are duplicate values in the dataset, this can be fixed using dict \"\"\"\n",
    "    result = {} # sets up dict \n",
    "    for name,value in tuples: # iterate through list\n",
    "        total = result.get(name,0) + value # find sum of duplicate values\n",
    "        result[name] = total\n",
    "\n",
    "    \"\"\" Final output -- Names and Values of equal len \"\"\"\n",
    "    # below lists should be of equal len\n",
    "    cleanNames = [a for a in result.keys()] # names of the companies  \n",
    "    cleanValues = [a for a in result.values()] # pct book values \n",
    "\n",
    "    \"\"\" This part matches the raw names from SEC to a list of all NYSE and NASDAQ companies and tickers using NLP \"\"\"\n",
    "    # this re stuff is needed for the NLP below. note, this does not have to be in function, can be somewhere else, just here for ease and clarity. \n",
    "    def ngrams(string, n=3):\n",
    "        string = string.encode(\"ascii\", errors=\"ignore\").decode() \n",
    "        string = string.lower()\n",
    "        chars_to_remove = [')', '(', '.', '|', '[', ']', '{', '}', \"'\"]\n",
    "        rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "        string = re.sub(rx, '', string) # remove the list of chars defined above\n",
    "        string = string.replace('&', 'and')\n",
    "        string = string.replace(',', ' ').replace('-', ' ')\n",
    "        string = string.title() # Capital at start of each word\n",
    "        string = re.sub(' +',' ',string).strip() # combine whitespace\n",
    "        string = ' ' + string + ' ' # pad\n",
    "        string = re.sub(r'[,-./]|\\sBD', r'', string)\n",
    "        ngrams = zip(*[string[i:] for i in range(n)])\n",
    "        return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "    # pulls data from the NYSE/NASDAQ list for names of companies. \n",
    "    exchangeNames = combEx['Name'].to_list() # this is list2, list1 is the raw names from SEC, important to note that this must be interchanged, can be imported from csv, etc. \n",
    "\n",
    "    # this is the NLP stuff that finds the matches. \n",
    "    \"\"\"For each item in list1, find the match in list2\"\"\"\n",
    "    list1 = cleanNames # these are so the code can be adjusted easier for other list inputs. List 1/2 are referenced in the code below. names from the input \n",
    "    list2 = exchangeNames # names to matched against, from exchanges \n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer=ngrams, lowercase=False)\n",
    "    tfidf = vectorizer.fit_transform(list2)\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\n",
    "    distances, indices = nbrs.kneighbors(vectorizer.transform(list1))\n",
    "    \n",
    "    matches = [(round(distances[i][0], 2), list1[i], list2[j[0]]) for i, j in enumerate(indices)]\n",
    "    matches = pd.DataFrame(matches, \n",
    "                           columns=['score', 'original', 'matched'])\n",
    "    \n",
    "    \"\"\" This code finds the matching ticker to the 'matched' cleaned name from the exchange dataframe. \"\"\"\n",
    "    matches['bookPct'] = cleanValues\n",
    "    matches = matches.loc[matches['score'] <= 0.70]\n",
    "    resDf = pd.DataFrame({'matches':matches['matched'].to_list(),'bookPct':matches['bookPct'].to_list()})\n",
    "    refTickers = combEx[['Name','Symbol']]\n",
    "    resDf = resDf.join(refTickers.set_index('Name'),on='matches')\n",
    "    resDf = resDf.drop(['matches'],axis=1)\n",
    "    resDf = resDf.reindex(columns=['Symbol','bookPct'])\n",
    "    return resDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(124, 2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Symbol  bookPct\n",
       "0   ONEM    0.001\n",
       "1   ACCD    0.002\n",
       "2   ADAP    0.000\n",
       "3   ADPT    0.006\n",
       "4   ADMA    0.006"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Symbol</th>\n      <th>bookPct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ONEM</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ACCD</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ADAP</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ADPT</td>\n      <td>0.006</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ADMA</td>\n      <td>0.006</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 298
    }
   ],
   "source": [
    "test = pullSec2(testUrl) # this is just a test of the functions ability to pull names, and bookPct\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 299
    }
   ],
   "source": [
    "len(test['Symbol'].to_list()) == len(test['Symbol'].unique().tolist()) # this means that there are no duplicate values in the dataframe. -- the update to the function was successful"
   ]
  },
  {
   "source": [
    "## The above code works"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The above function works, it removes duplicates, names associated with puts, and calculates the bookPct of each positions before these eliminations. This means that the book Pct really represents the actual conviction of the fund and not a subset. This dataset represents only the tickers that they are focusing on for long exposure. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The code below are urls and periods. This list will be iterated through. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the list that contains the string with the date of the event and the url to the xml edgar page\n",
    "# note that this particular proof of concept is for Perc Adv -- this is because their book is the most complex, other ones should be easy compared to this one. \n",
    "\n",
    "\"\"\" This is the program that creates the list the stores the periods and urls that will be scraped. \"\"\"\n",
    "# note the text to the right of each line is the period and the date of filing. -- once again all these urls are for perc adv\n",
    "\n",
    "urls = [\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297520000835/xslForm13F_X01/infotable.xml', #q320 - 2020-11-16\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297520000701/xslForm13F_X01/infotable.xml', #q220 - 2020-08-17\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297520000472/xslForm13F_X01/infotable.xml', #q120 - 2020-05-18\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297520000250/xslForm13F_X01/perceptive.xml', #q419 - 2020-02-19\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297519000707/xslForm13F_X01/infotable.xml', #q319 - 2019-11-14\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297519000583/xslForm13F_X01/infotable.xml', #q219 - 2019-08-14\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297519000380/xslForm13F_X01/infotable.xml', #q119 - 2019-05-16\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297519000177/xslForm13F_X01/infotable.xml', #q418 - 2019-02-14\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297518001239/xslForm13F_X01/infotable.xml', #q318 - 2018-11-14\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297518001030/xslForm13F_X01/infotable.xml', #q218 - 2018-08-14\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297519000380/xslForm13F_X01/infotable.xml', #q118 - 2018-05-15\n",
    "    'https://www.sec.gov/Archives/edgar/data/1224962/000101297518000171/xslForm13F_X01/infotable.xml' #q417 - 2018-02-14\n",
    "    ] \n",
    "\n",
    "periods = [ # this is the list of periods that are covered in the analysis. -- this is the same for every fund. \n",
    "    'Q320',\n",
    "    'Q220',\n",
    "    'Q120',\n",
    "    'Q419',\n",
    "    'Q319',\n",
    "    'Q219',\n",
    "    'Q119',\n",
    "    'Q418',\n",
    "    'Q318',\n",
    "    'Q218',\n",
    "    'Q118',\n",
    "    'Q417'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the goal of this cell is to create a way to iterate over an unknown number \n",
    "\n",
    "\"\"\" Note that this requires the previous function pullSec \"\"\"\n",
    "def createCsv2(periods, urls):\n",
    "    res = [] # this is just an empty list that the dataframes can be stored in\n",
    "    for a in urls: # this iteration pulls a total list of all the tickers over that period of time, book value is dropped. \n",
    "        data = pullSec2(a) # this references the function that is defined earlier. \n",
    "        res.append(data)\n",
    "\n",
    "    tickers = []\n",
    "    for a in res: # this references each res df that is present in the master list \n",
    "        ticker = a['Symbol'].to_list()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    flatTickers = [item for sublist in tickers for item in sublist] # flattens the list of tickers \n",
    "    finalTickers = np.unique(np.array(flatTickers)).tolist() # removes the duplicates from the list\n",
    "    masterDf = pd.DataFrame({'Symbol':finalTickers}) # this is the master dataframe that each fq will be matched against with the book values -- unique values\n",
    "    for a in [a for a in range(len(periods))]: # this renames each bookPct col to be the corresponding period\n",
    "        res[a].columns = ['Symbol',periods[a]]\n",
    "    res.insert(0,masterDf) # this just inserts the dataframe into the 0 position.\n",
    "    dfs = [df.set_index('Symbol') for df in res]\n",
    "    output = dfs[0].join(dfs[1:])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = createCsv2(periods,urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Q320  Q220  Q120   Q419   Q319   Q219   Q119   Q418   Q318   Q218  \\\n",
       "Symbol                                                                       \n",
       "ABEO      NaN   NaN   0.0  0.002  0.002  0.001  0.003  0.003  0.003  0.001   \n",
       "ABEO      NaN   NaN   0.0  0.002  0.002  0.001  0.003  0.003  0.003  0.002   \n",
       "ABMD      NaN   NaN   NaN    NaN    NaN    NaN    NaN    NaN  0.000  0.000   \n",
       "ACAD      NaN   NaN   NaN  0.000    NaN    NaN  0.000    NaN    NaN    NaN   \n",
       "ACCD    0.002   NaN   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "         Q118   Q417  \n",
       "Symbol                \n",
       "ABEO    0.003  0.003  \n",
       "ABEO    0.003  0.003  \n",
       "ABMD      NaN  0.000  \n",
       "ACAD    0.000    NaN  \n",
       "ACCD      NaN    NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Q320</th>\n      <th>Q220</th>\n      <th>Q120</th>\n      <th>Q419</th>\n      <th>Q319</th>\n      <th>Q219</th>\n      <th>Q119</th>\n      <th>Q418</th>\n      <th>Q318</th>\n      <th>Q218</th>\n      <th>Q118</th>\n      <th>Q417</th>\n    </tr>\n    <tr>\n      <th>Symbol</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ABEO</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>ABEO</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.002</td>\n      <td>0.003</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>ABMD</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>ACAD</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ACCD</th>\n      <td>0.002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 303
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' This shows that increasing the accuracy matching benchmark actually removes non-duplicates from the dataset but is not able to remove the ABEO duplicate '"
      ]
     },
     "metadata": {},
     "execution_count": 304
    }
   ],
   "source": [
    "output.shape # 312, 12 @ 0.70-- this is the shape of the df before the accuracy was increased (309,12 @ 0.65 ) \n",
    "\"\"\" This shows that increasing the accuracy matching benchmark actually removes non-duplicates from the dataset but is not able to remove the ABEO duplicate \"\"\""
   ]
  },
  {
   "source": [
    "When the matching percent was decreased to 0.65, (makes the barrier for inclusion higher), the shape of the df is decreased by 3 rows, note that before the only duplicate datapoints are the ABEO rows, this means that lowering this benchmark had decreased the number of unique inclusions in the result. This is a problem. Further, the ABEO duplicate datapoint is still present. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Q320   Q220   Q120   Q419   Q319   Q219   Q119   Q418   Q318   Q218  \\\n",
       "Symbol                                                                         \n",
       "ABEO      NaN    NaN  0.000  0.002  0.002  0.001  0.003  0.003  0.003  0.001   \n",
       "ABEO      NaN    NaN  0.000  0.002  0.002  0.001  0.003  0.003  0.003  0.002   \n",
       "ABMD      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN  0.000  0.000   \n",
       "ACAD      NaN    NaN    NaN  0.000    NaN    NaN  0.000    NaN    NaN    NaN   \n",
       "ACCD    0.002    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "ZBH       NaN    NaN  0.001  0.000  0.002    NaN    NaN    NaN    NaN    NaN   \n",
       "ZGNX      NaN  0.003  0.000  0.004  0.001  0.003  0.024  0.002  0.002  0.030   \n",
       "ZIOP      NaN    NaN    NaN    NaN    NaN  0.002    NaN    NaN    NaN    NaN   \n",
       "ZNTL    0.005  0.006    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "ZYME    0.030  0.005  0.001  0.001  0.000  0.024  0.003  0.005  0.006    NaN   \n",
       "\n",
       "         Q118   Q417  \n",
       "Symbol                \n",
       "ABEO    0.003  0.003  \n",
       "ABEO    0.003  0.003  \n",
       "ABMD      NaN  0.000  \n",
       "ACAD    0.000    NaN  \n",
       "ACCD      NaN    NaN  \n",
       "...       ...    ...  \n",
       "ZBH       NaN    NaN  \n",
       "ZGNX    0.024    NaN  \n",
       "ZIOP      NaN    NaN  \n",
       "ZNTL      NaN    NaN  \n",
       "ZYME    0.003    NaN  \n",
       "\n",
       "[312 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Q320</th>\n      <th>Q220</th>\n      <th>Q120</th>\n      <th>Q419</th>\n      <th>Q319</th>\n      <th>Q219</th>\n      <th>Q119</th>\n      <th>Q418</th>\n      <th>Q318</th>\n      <th>Q218</th>\n      <th>Q118</th>\n      <th>Q417</th>\n    </tr>\n    <tr>\n      <th>Symbol</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ABEO</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>ABEO</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.002</td>\n      <td>0.003</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>ABMD</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>ACAD</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ACCD</th>\n      <td>0.002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>ZBH</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ZGNX</th>\n      <td>NaN</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.004</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.024</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.030</td>\n      <td>0.024</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ZIOP</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ZNTL</th>\n      <td>0.005</td>\n      <td>0.006</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>ZYME</th>\n      <td>0.030</td>\n      <td>0.005</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.024</td>\n      <td>0.003</td>\n      <td>0.005</td>\n      <td>0.006</td>\n      <td>NaN</td>\n      <td>0.003</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>312 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 305
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the sum of pct book each q \n",
    "theSum = output.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Q320    0.996\n",
       "Q220    0.903\n",
       "Q120    0.858\n",
       "Q419    0.900\n",
       "Q319    0.791\n",
       "Q219    0.846\n",
       "Q119    0.802\n",
       "Q418    0.803\n",
       "Q318    0.701\n",
       "Q218    0.767\n",
       "Q118    0.802\n",
       "Q417    0.748\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 307
    }
   ],
   "source": [
    "theSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Q320    0.008\n",
       "Q220    0.007\n",
       "Q120    0.008\n",
       "Q419    0.008\n",
       "Q319    0.008\n",
       "Q219    0.009\n",
       "Q119    0.008\n",
       "Q418    0.009\n",
       "Q318    0.007\n",
       "Q218    0.008\n",
       "Q118    0.008\n",
       "Q417    0.008\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 308
    }
   ],
   "source": [
    "mean = output.mean(axis=0).round(decimals=3) # this is the mean pct book allocated to each position for each quarter \n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "noNan = output[~output.isnull().any(axis=1)] # this is a df of only rows with no nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(18, 12)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Q320   Q220   Q120   Q419   Q319   Q219   Q119   Q418   Q318   Q218  \\\n",
       "Symbol                                                                         \n",
       "ADMA    0.006  0.006  0.007  0.007  0.006  0.006  0.006  0.006  0.011  0.007   \n",
       "ALBO    0.011  0.011  0.007  0.007  0.000  0.000  0.007  0.011  0.006  0.001   \n",
       "ALDX    0.007  0.007  0.006  0.006  0.011  0.011  0.006  0.007  0.001  0.001   \n",
       "ARNA    0.007  0.024  0.008  0.001  0.024  0.007  0.008  0.001  0.001  0.000   \n",
       "BPMC    0.009  0.001  0.001  0.010  0.037  0.037  0.024  0.037  0.010  0.032   \n",
       "CTMX    0.003  0.003  0.008  0.032  0.008  0.003  0.003  0.003  0.003  0.009   \n",
       "EIGR    0.000  0.002  0.000  0.005  0.000  0.002  0.005  0.002  0.005  0.003   \n",
       "FOLD    0.052  0.052  0.001  0.001  0.001  0.052  0.001  0.007  0.007  0.005   \n",
       "GBT     0.047  0.002  0.009  0.000  0.009  0.002  0.003  0.002  0.002  0.003   \n",
       "IOVA    0.048  0.048  0.047  0.004  0.002  0.003  0.004  0.000  0.048  0.005   \n",
       "KDMN    0.009  0.009  0.002  0.007  0.002  0.047  0.007  0.003  0.009  0.012   \n",
       "MTEM    0.004  0.006  0.002  0.000  0.002  0.001  0.000  0.007  0.005  0.021   \n",
       "MYOK    0.028  0.000  0.005  0.005  0.001  0.009  0.001  0.048  0.004  0.021   \n",
       "NBIX    0.022  0.003  0.083  0.004  0.012  0.001  0.005  0.009  0.002  0.001   \n",
       "RACE    0.000  0.009  0.001  0.002  0.001  0.002  0.000  0.002  0.009  0.001   \n",
       "RTRX    0.066  0.008  0.011  0.000  0.021  0.004  0.017  0.004  0.014  0.005   \n",
       "SRRA    0.000  0.003  0.012  0.014  0.003  0.022  0.000  0.006  0.057  0.003   \n",
       "VBIV    0.023  0.002  0.024  0.003  0.000  0.057  0.001  0.000  0.001  0.003   \n",
       "\n",
       "         Q118   Q417  \n",
       "Symbol                \n",
       "ADMA    0.006  0.007  \n",
       "ALBO    0.007  0.052  \n",
       "ALDX    0.006  0.001  \n",
       "ARNA    0.008  0.009  \n",
       "BPMC    0.024  0.016  \n",
       "CTMX    0.003  0.003  \n",
       "EIGR    0.005  0.001  \n",
       "FOLD    0.001  0.005  \n",
       "GBT     0.003  0.009  \n",
       "IOVA    0.004  0.083  \n",
       "KDMN    0.007  0.001  \n",
       "MTEM    0.000  0.000  \n",
       "MYOK    0.001  0.021  \n",
       "NBIX    0.005  0.001  \n",
       "RACE    0.000  0.009  \n",
       "RTRX    0.017  0.003  \n",
       "SRRA    0.000  0.000  \n",
       "VBIV    0.001  0.003  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Q320</th>\n      <th>Q220</th>\n      <th>Q120</th>\n      <th>Q419</th>\n      <th>Q319</th>\n      <th>Q219</th>\n      <th>Q119</th>\n      <th>Q418</th>\n      <th>Q318</th>\n      <th>Q218</th>\n      <th>Q118</th>\n      <th>Q417</th>\n    </tr>\n    <tr>\n      <th>Symbol</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ADMA</th>\n      <td>0.006</td>\n      <td>0.006</td>\n      <td>0.007</td>\n      <td>0.007</td>\n      <td>0.006</td>\n      <td>0.006</td>\n      <td>0.006</td>\n      <td>0.006</td>\n      <td>0.011</td>\n      <td>0.007</td>\n      <td>0.006</td>\n      <td>0.007</td>\n    </tr>\n    <tr>\n      <th>ALBO</th>\n      <td>0.011</td>\n      <td>0.011</td>\n      <td>0.007</td>\n      <td>0.007</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.011</td>\n      <td>0.006</td>\n      <td>0.001</td>\n      <td>0.007</td>\n      <td>0.052</td>\n    </tr>\n    <tr>\n      <th>ALDX</th>\n      <td>0.007</td>\n      <td>0.007</td>\n      <td>0.006</td>\n      <td>0.006</td>\n      <td>0.011</td>\n      <td>0.011</td>\n      <td>0.006</td>\n      <td>0.007</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.006</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>ARNA</th>\n      <td>0.007</td>\n      <td>0.024</td>\n      <td>0.008</td>\n      <td>0.001</td>\n      <td>0.024</td>\n      <td>0.007</td>\n      <td>0.008</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.009</td>\n    </tr>\n    <tr>\n      <th>BPMC</th>\n      <td>0.009</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.010</td>\n      <td>0.037</td>\n      <td>0.037</td>\n      <td>0.024</td>\n      <td>0.037</td>\n      <td>0.010</td>\n      <td>0.032</td>\n      <td>0.024</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>CTMX</th>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.008</td>\n      <td>0.032</td>\n      <td>0.008</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.009</td>\n      <td>0.003</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>EIGR</th>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.005</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.005</td>\n      <td>0.002</td>\n      <td>0.005</td>\n      <td>0.003</td>\n      <td>0.005</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>FOLD</th>\n      <td>0.052</td>\n      <td>0.052</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.052</td>\n      <td>0.001</td>\n      <td>0.007</td>\n      <td>0.007</td>\n      <td>0.005</td>\n      <td>0.001</td>\n      <td>0.005</td>\n    </tr>\n    <tr>\n      <th>GBT</th>\n      <td>0.047</td>\n      <td>0.002</td>\n      <td>0.009</td>\n      <td>0.000</td>\n      <td>0.009</td>\n      <td>0.002</td>\n      <td>0.003</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.009</td>\n    </tr>\n    <tr>\n      <th>IOVA</th>\n      <td>0.048</td>\n      <td>0.048</td>\n      <td>0.047</td>\n      <td>0.004</td>\n      <td>0.002</td>\n      <td>0.003</td>\n      <td>0.004</td>\n      <td>0.000</td>\n      <td>0.048</td>\n      <td>0.005</td>\n      <td>0.004</td>\n      <td>0.083</td>\n    </tr>\n    <tr>\n      <th>KDMN</th>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>0.002</td>\n      <td>0.007</td>\n      <td>0.002</td>\n      <td>0.047</td>\n      <td>0.007</td>\n      <td>0.003</td>\n      <td>0.009</td>\n      <td>0.012</td>\n      <td>0.007</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>MTEM</th>\n      <td>0.004</td>\n      <td>0.006</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.005</td>\n      <td>0.021</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>MYOK</th>\n      <td>0.028</td>\n      <td>0.000</td>\n      <td>0.005</td>\n      <td>0.005</td>\n      <td>0.001</td>\n      <td>0.009</td>\n      <td>0.001</td>\n      <td>0.048</td>\n      <td>0.004</td>\n      <td>0.021</td>\n      <td>0.001</td>\n      <td>0.021</td>\n    </tr>\n    <tr>\n      <th>NBIX</th>\n      <td>0.022</td>\n      <td>0.003</td>\n      <td>0.083</td>\n      <td>0.004</td>\n      <td>0.012</td>\n      <td>0.001</td>\n      <td>0.005</td>\n      <td>0.009</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.005</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>RACE</th>\n      <td>0.000</td>\n      <td>0.009</td>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.009</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.009</td>\n    </tr>\n    <tr>\n      <th>RTRX</th>\n      <td>0.066</td>\n      <td>0.008</td>\n      <td>0.011</td>\n      <td>0.000</td>\n      <td>0.021</td>\n      <td>0.004</td>\n      <td>0.017</td>\n      <td>0.004</td>\n      <td>0.014</td>\n      <td>0.005</td>\n      <td>0.017</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>SRRA</th>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.012</td>\n      <td>0.014</td>\n      <td>0.003</td>\n      <td>0.022</td>\n      <td>0.000</td>\n      <td>0.006</td>\n      <td>0.057</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>VBIV</th>\n      <td>0.023</td>\n      <td>0.002</td>\n      <td>0.024</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.057</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>0.003</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 310
    }
   ],
   "source": [
    "print(noNan.shape) # this is a very small subset of the data, indicates the perc rarely increases q/q\n",
    "noNan # this is the rows of the dataframe where there are no nan values"
   ]
  },
  {
   "source": [
    "The small percentage of the total companies that have no NaN values in any FQ indicate that there are few companies that Perc Adv have strong Q/Q increases in over a long period. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 311
    }
   ],
   "source": [
    "len(output.index) == len(output.index.unique()) # means that there is a duplicate in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 312
    }
   ],
   "source": [
    "len(output.index) - len(output.index.unique()) # this is the difference between the len of the index and unique values, if the len of the lists are equal then there are no duplicates, otherwise there are duplicates, this shows how many duplicates there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" simple function to find duplicates in the dataset \"\"\"\n",
    "unique = [] \n",
    "duplicates = []\n",
    "for a in output.index.tolist():\n",
    "    if a not in unique:\n",
    "        unique.append(a)\n",
    "    else:\n",
    "        duplicates.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ABEO']"
      ]
     },
     "metadata": {},
     "execution_count": 314
    }
   ],
   "source": [
    "duplicates # this is the duplicate value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('PercAdvQ-Q113s.csv') # saves the final output as .csv for further analysis "
   ]
  },
  {
   "source": [
    "## Conclusions\n",
    "This code represents a proof of concept for pulling multiple quarters of SEC filings and compiling them into a single dataset for further analysis. This is an imperfect method that would be much improved with access to CUSIP datasets. There are some duplicates in the final dataset likely due to the NLP TDIFD matching but this is a problem I will continue to work. The project overall is a success as this simple datastructure can be exported to .csv and then imported into other code for further analysis. The hardcoded urls and periods used in this code could likely be automated using a web crawler. That is really out of the scope of this project as this is just proof of concept. However, it warrants further research. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}